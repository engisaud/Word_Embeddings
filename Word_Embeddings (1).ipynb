{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as continuous vectors in a high-dimensional space. Unlike traditional representations like Bag of Words (BoW), word embeddings capture semantic meanings and relationships between words by placing similar words closer together in the vector space.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Word Embedding**: A dense vector representation of a word where each dimension captures some aspect of its meaning.\n",
        "2. **Pre-trained Embeddings**: Embeddings learned from large corpora, such as Word2Vec, GloVe, and FastText.\n",
        "3. **Semantic Similarity**: Words with similar meanings will have similar embeddings, making it easier to perform tasks like word similarity and analogy."
      ],
      "metadata": {
        "id": "ihBopOobLb2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n"
      ],
      "metadata": {
        "id": "J14EKzDQLg2f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load pre-trained Word2Vec model (Google News vectors)\n",
        "Note: This model is quite large. For demonstration, use a smaller or different model as needed.\n",
        " model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "6YBv0QyqLiSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, we'll use a smaller pre-trained model available in gensim\n",
        "from gensim.downloader import load\n",
        "model = load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "Iyp8HlpbLmHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d1a9b9-3584-468b-b424-7e4344d909f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "words = ['king', 'queen', 'man', 'woman']"
      ],
      "metadata": {
        "id": "xkigRBgHLoiP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "embeddings = {word: model[word] for word in words}"
      ],
      "metadata": {
        "id": "xJngiE2qLqOr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H01quYFFLWE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd35bc9-1c65-4a74-cbfd-fd79aa081f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: king\n",
            "Embedding: [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "\n",
            "Word: queen\n",
            "Embedding: [ 0.37854    1.8233    -1.2648    -0.1043     0.35829    0.60029\n",
            " -0.17538    0.83767   -0.056798  -0.75795    0.22681    0.98587\n",
            "  0.60587   -0.31419    0.28877    0.56013   -0.77456    0.071421\n",
            " -0.5741     0.21342    0.57674    0.3868    -0.12574    0.28012\n",
            "  0.28135   -1.8053    -1.0421    -0.19255   -0.55375   -0.054526\n",
            "  1.5574     0.39296   -0.2475     0.34251    0.45365    0.16237\n",
            "  0.52464   -0.070272  -0.83744   -1.0326     0.45946    0.25302\n",
            " -0.17837   -0.73398   -0.20025    0.2347    -0.56095   -2.2839\n",
            "  0.0092753 -0.60284  ]\n",
            "\n",
            "Word: man\n",
            "Embedding: [-0.094386  0.43007  -0.17224  -0.45529   1.6447    0.40335  -0.37263\n",
            "  0.25071  -0.10588   0.10778  -0.10848   0.15181  -0.65396   0.55054\n",
            "  0.59591  -0.46278   0.11847   0.64448  -0.70948   0.23947  -0.82905\n",
            "  1.272     0.033021  0.2935    0.3911   -2.8094   -0.70745   0.4106\n",
            "  0.3894   -0.2913    2.6124   -0.34576  -0.16832   0.25154   0.31216\n",
            "  0.31639   0.12539  -0.012646  0.22297  -0.56585  -0.086264  0.62549\n",
            " -0.0576    0.29375   0.66005  -0.53115  -0.48233  -0.97925   0.53135\n",
            " -0.11725 ]\n",
            "\n",
            "Word: woman\n",
            "Embedding: [-1.8153e-01  6.4827e-01 -5.8210e-01 -4.9451e-01  1.5415e+00  1.3450e+00\n",
            " -4.3305e-01  5.8059e-01  3.5556e-01 -2.5184e-01  2.0254e-01 -7.1643e-01\n",
            "  3.0610e-01  5.6127e-01  8.3928e-01 -3.8085e-01 -9.0875e-01  4.3326e-01\n",
            " -1.4436e-02  2.3725e-01 -5.3799e-01  1.7773e+00 -6.6433e-02  6.9795e-01\n",
            "  6.9291e-01 -2.6739e+00 -7.6805e-01  3.3929e-01  1.9695e-01 -3.5245e-01\n",
            "  2.2920e+00 -2.7411e-01 -3.0169e-01  8.5286e-04  1.6923e-01  9.1433e-02\n",
            " -2.3610e-02  3.6236e-02  3.4488e-01 -8.3947e-01 -2.5174e-01  4.2123e-01\n",
            "  4.8616e-01  2.2325e-02  5.5760e-01 -8.5223e-01 -2.3073e-01 -1.3138e+00\n",
            "  4.8764e-01 -1.0467e-01]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display embeddings\n",
        "for word, vector in embeddings.items():\n",
        "    print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.most_similar('computer', topn=5)\n",
        "\n",
        "# TODO:: Display similar words\n",
        "print(similar_words)"
      ],
      "metadata": {
        "id": "rKCazO0NLsOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b07d970-ebb2-4440-cf32-f80909e1db94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('computers', 0.9165045022964478), ('software', 0.8814992904663086), ('technology', 0.852556049823761), ('electronic', 0.812586784362793), ('internet', 0.8060455322265625)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve analogy\n",
        "analogy_result = model.most_similar(positive=['queen', 'man'], negative=['king'], topn=1)\n",
        "\n",
        "# TODO:: Display result\n",
        "print(analogy_result)"
      ],
      "metadata": {
        "id": "fkcpJPKvL43C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8bd86a-9897-4ea2-a418-b75f0aad54eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8947036266326904)]\n"
          ]
        }
      ]
    }
  ]
}